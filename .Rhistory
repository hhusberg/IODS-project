model1 <- lm(points ~ attitude + deep + surf, data = learning2014)
learning2014 <- read.csv("./data/learning2014.csv")
head(learning2014)
dim(learning2014)
summary(learning2014)
model1 <- lm(points ~ attitude + deep + surf, data = learning2014)
summary(model1)
?ggpairs
library(dplyr)
library(GGally)
?ggpairs
summary(alc$sex)
library(dplyr)
library(GGally)
alc <- read.csv("./data/alc_data.csv")
glimpse(alc)
summary(alc$sex)
summary(alc$age)
summary(alc$internet)
summary(alc$failures)
g1 <- ggplot(data=alc, aes(x = high_use, fill = sex))
library(dplyr)
library(GGally)
alc <- read.csv("./data/alc_data.csv")
glimpse(alc)
g1 <- ggplot(data=alc, aes(x = high_use, fill = sex))
library(dplyr)
library(GGally)
library(ggplot2)
g1 <- ggplot(data=alc, aes(x = high_use, fill = sex))
g1 <- ggplot(data=alc, aes(x = high_use, fill = sex))
g1 + geom_bar()
probabilities <- predict(my_glmodel2, type = "response")
probabilities <- predict(my_glmodel2, type = "response")
library(dplyr)
library(GGally)
library(ggplot2)
alc <- read.csv("./data/alc_data.csv")
glimpse(alc)
summary(alc$sex)
summary(alc$age)
summary(alc$internet)
summary(alc$failures)
g1 <- ggplot(data=alc, aes(x = high_use, fill = sex))
g1 + geom_bar()
g2 <- ggplot(data=alc, aes(x = high_use, y = age, col = sex))
g2 + geom_jitter()
table(alc$high_use, alc$internet)
my_glmodel <- glm(high_use ~ sex + age + internet + failures, data = alc, family="binomial")
summary(my_glmodel)
table(alc$high_use, alc$failures)
OR <- coef(my_glmodel) %>% exp
CI <- confint(my_glmodel) %>% exp
cbind(OR, CI)
my_glmodel2 <- glm(high_use ~ sex + failures, data = alc, family = "binomial")
summary(my_glmodel2)
coef(my_glmodel2)
probabilities <- predict(my_glmodel2, type = "response")
alc <- mutate(alc, probability = probabilities)
alc <- mutate(alc, prediction = probability > 0.5)
table(high_use = alc$high_use, prediction = alc$prediction)
g3 <- ggplot(alc, aes(x = probability, y = high_use, col = prediction))
g3 + geom_point()
g3 + geom_jitter()
g3 + geom_point()
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class=alc$high_use, alc$probability)
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = my_glmodel2, K = 10)
cv$delta[1]
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = my_glmodel2, K = 10)
cv$delta[1]
library(MASS)
library(dplyr)
library(GGally)
library(ggplot2)
str(Boston)
dim(Boston)
pairs(Boston)
pairs(data=Boston, main="Simple Scatterplot Matric")
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
library(MASS)
library(dplyr)
library(GGally)
library(ggplot2)
library(corrplot)
install.packages("corrplot")
library(corrplot)
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
str(Boston)
dim(Boston)
summary(Boston$crim)
summary(Boston$tax)
str(Boston)
dim(Boston)
summary(Boston$crim)
summary(Boston$tax)
myboston_scaled <- scale(Boston)
summary(myboston_scaled)
?scale
cutoffs <- quantile(myboston_scaled$crim)
class(myboston_scaled)
myboston_scaled <- as.data.frame(myboston_scaled)
cutoffs <- quantile(myboston_scaled$crim)
cutoffs
cutoffs <- quantile(myboston_scaled$crim)
labels <- c("low", "mod_low", "mod_high", "high")
crim_cat <- cut(myboston_scaled$crim, breaks=cutoffs, include.lowest = TRUE, label = labels)
table(crim_cat)
boston_scaled <- dplyr::select(myboston_scaled, -crim)
str(myboston_scaled)
myboston_scaled <- dplyr::select(myboston_scaled, -crim)
str(myboston_scaled)
myboston_scaled <- data.frame(myboston_scaled, crim_cat)
str(myboston_scaled)
n <- nrow(myboston_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- myboston_scaled[eighty_perc,]
test <- myboston_scaled[-eighty_perc,]
lda.fit <- lda(crim_cat ~ ., data = train)
lda.fit
lda.fit <- lda(crim_cat ~ ., data = train)
lda.fit <- lda(crim_cat ~ ., data = train)
# Chapter 4 - Clustering and classification
```{r echo = FALSE, results='hide', message=FALSE}
library(MASS)
library(dplyr)
library(GGally)
library(ggplot2)
library(corrplot)
```
Libraries loaded, let's get clustering. First up we'll look at the structure and dimensions of the Boston data, a dataframe generously provided as part of the R package MASS.If you want to know more about this dataset have a look [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). Now a quick overview of the data:
```{r}
str(Boston)
dim(Boston)
summary(Boston$crim)
summary(Boston$tax)
```
So in brief, this data describes housing values in the suburbs of Boston. We have 14 variables and 506 observations.The variable _crim_ refers to per capita crime rate and will be our primary variable of interest. Examples of other variables are _chas_, a dichotomous variable describing whether the area bounds the river Charles or not; _rm_, the average number of rooms per dwelling in the area and _ptratio_, the ratio of pupils to teacher. Check out the link above for full details on the variables.
As is obvious from the summary above, the scales of these variables are very dissimilar (I used _crim_ and _tax_ to illustrate this). This is not a problem as long as we're looking at correlations, where they're standardized anyway, but later we'll standardize them for our other analyses.
Now for a quick look at how these variables interact:
```{r}
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```
From this, we can tell that some of the variables seem to be quite strongly related and that not very many have anything close to a normal distribution. So let's have a look at the correlation matrix to get some figures for those relationships. Below the correlation matrix I'll do a correlation plot to illustrate these figures, because I can:
```{r}
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```
As could be assumed from the scatterplots many of these variables are quite strongly correlated. So for example there is a correlation of .63 between the variables _crim_, which is the crime rate, and _rad_, which is the index of acessibility to radial highways. Highway robbery? Go figure.
There's a few rather obvious relationships, for example the level of industry _(indus)_ is positively correlated with the level of air pollution _(nox)_. To read more about the dangers of nitrogen oxide, see [Wikipedia](https://en.wikipedia.org/wiki/NOx).
Now we'll standardize the dataset. If I remember to do so I'll look up an easy explanation for why this needs to be done and _insert it here_. If it's still missing at the time of assessment, assume I know but forgot to update my diary...
To scale the whole dataset at once, we'll do this:
```{r}
myboston_scaled <- scale(Boston)
summary(myboston_scaled)
myboston_scaled <- as.data.frame(myboston_scaled)
```
That was easy. Now the variables all have a mean of zero and a standard deviation of one. Now we're to create a categorical crime rate variable using quantiles as break points. This will allow us to (arbitrarily) define crime rates as low, moderate or high.
```{r}
cutoffs <- quantile(myboston_scaled$crim)
labels <- c("low", "mod_low", "mod_high", "high")
crim_cat <- cut(myboston_scaled$crim, breaks=cutoffs, include.lowest = TRUE, label = labels)
table(crim_cat)
```
So there we have them tabled, obiously (approximately) the same amount of observations per quantile. Oh, now we have to get rid of the old, continuous _crim_-variable and add the new one:
```{r}
myboston_scaled <- dplyr::select(myboston_scaled, -crim)
myboston_scaled <- data.frame(myboston_scaled, crim_cat)
str(myboston_scaled)
```
Looks good. Now this dataset needs to be divided into a __training__ set and a __testing__ set. To do that we'll use the function _nrow()_ to get the number of rows and then randomly select 80% of those for our training set:
```{r}
n <- nrow(myboston_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- myboston_scaled[eighty_perc,]
test <- myboston_scaled[-eighty_perc,]
```
Now it's time for some LDA. First we'll compute the LDA fit using all the variables in the dataset:
```{r}
lda.fit <- lda(crim_cat ~ ., data = train)
lda.fit
```
Then we'll draw a biplot:
```{r}
lda.arrows <- function(x, myscale = 0.5, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crim_cat)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```
Next we'll save the classes from the test set as a new vector and remove them from the test set:
```{r}
correct_classes <- test$crim_cat
test <- dplyr::select(test, -crim_cat)
```
And finally, we'll use our LDA-model to predict classes for the test set and compare those to the correct classes by cross-tabulating:
```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```
So our model predicted the crime rates rather well. A quick calculation gives us a total of
```{r}
total <- c(18+17+14+26+5+1+7+7)
total
```
observations of which
```{r}
corr <- c(18+17+14+26)
corr
```
were correctly predicted while
```{r}
wrong <- c(5+1+7+7)
wrong
```
were incorrectly predicted, resulting in a prediction accuracy of
```{r}
ratio <- c(corr/total)
ratio
```
In conclusion our model was fairly accurate in predicting crime rates.
Now it's time to look at the clustering and distances. First up we'll reload the Boston data and scale it:
```{r}
new_Boston <- Boston
str(new_Boston)
new_Boston_scaled <- scale(new_Boston) %>% as.data.frame()
class(new_Boston_scaled)
str(new_Boston_scaled)
```
Then we'll get the Euclidean distance between observations:
```{r}
dist_eu <- dist(new_Boston_scaled)
summary(dist_eu)
head(dist_eu)
```
Then we will run k-means:
```{r}
km <-kmeans(dist_eu, centers=4)
pairs(new_Boston_scaled, col = km$cluster)
```
p <- ggpairs(myboston_scaled, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
learning2014 <- read.csv("./data/learning2014.csv")
head(learning2014)
library(GGally)
library(ggplot2)
p <- ggpairs(learning2014, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
install.packages("httpuv")
library(GGally)
library(ggplot2)
p <- qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
print(p)
library(GGally)
library(ggplot2)
p <- qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
print(p)
library(GGally)
library(ggplot2)
p <- qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
print(p)
library(GGally)
library(ggplot2)
p <- qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
print(p)
install.packages("caTools")
library(GGally)
library(ggplot2)
p <- qplot(attitude, points, data = learning2014) + geom_smooth(method = "lm")
print(p)
#Exercise 4 - Data Wrangling
library(plyr)
#Read datas:
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
#Explore datasets:
str(hd)
dim(hd)
str(gii)
dim(gii)
sum_hd <- summary(hd)
sum_gii <- summary(gii)
sum_hd
sum_gii
hdnew <- rename(hd, c("HDI.Rank"="rank", "Country"="country", "Human.Development.Index..HDI."="hdi", "Life.Expectancy.at.Birth"="lifeexp", "Expected.Years.of.Education"="eduexp", "Mean.Years.of.Education"="edumean", "Gross.National.Income..GNI..per.Capita"="gni", "GNI.per.Capita.Rank.Minus.HDI.Rank"="gnirank"))
names(hdnew)
giinew <- rename(gii, c("GII.Rank"="rank", "Country"="country", "Gender.Inequality.Index..GII."="gii", "Maternal.Mortality.Ratio"="matmort", "Adolescent.Birth.Rate"="ado_birth", "Percent.Representation.in.Parliament"="rep_parl", "Population.with.Secondary.Education..Female."="sec_ed_fem", "Population.with.Secondary.Education..Male."="sec_ed_mal", "Labour.Force.Participation.Rate..Female."="labour_fem", "Labour.Force.Participation.Rate..Male."="labour_mal"))
names(giinew)
giinew <- mutate(giinew, sec_ed_F2M = sec_ed_fem/sec_ed_mal, labour_F2M = labour_fem/labour_mal)
join_by <- c("country")
library(dplyr)
human <- inner_join(hdnew, giinew, by = c(join_by), suffix = c(".hd", ".gii"))
names(human)
dim(human)
names(human)
human$gni
human <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric))
human <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
library(stringr)
human$gni
human <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni
human <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human
human$gni
human
human$gni <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni
human$gni
library(stringr)
$gni
human$gni
human$gni <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni_num <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni
library(plyr)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(hd)
str(gii)
dim(gii)
sum_hd <- summary(hd)
sum_gii <- summary(gii)
sum_hd
sum_gii
hdnew <- rename(hd, c("HDI.Rank"="rank", "Country"="country", "Human.Development.Index..HDI."="hdi", "Life.Expectancy.at.Birth"="lifeexp", "Expected.Years.of.Education"="eduexp", "Mean.Years.of.Education"="edumean", "Gross.National.Income..GNI..per.Capita"="gni", "GNI.per.Capita.Rank.Minus.HDI.Rank"="gnirank"))
names(hdnew)
giinew <- rename(gii, c("GII.Rank"="rank", "Country"="country", "Gender.Inequality.Index..GII."="gii", "Maternal.Mortality.Ratio"="matmort", "Adolescent.Birth.Rate"="ado_birth", "Percent.Representation.in.Parliament"="rep_parl", "Population.with.Secondary.Education..Female."="sec_ed_fem", "Population.with.Secondary.Education..Male."="sec_ed_mal", "Labour.Force.Participation.Rate..Female."="labour_fem", "Labour.Force.Participation.Rate..Male."="labour_mal"))
names(giinew)
giinew <- mutate(giinew, sec_ed_F2M = sec_ed_fem/sec_ed_mal, labour_F2M = labour_fem/labour_mal)
join_by <- c("country")
library(dplyr)
human <- inner_join(hdnew, giinew, by = c(join_by), suffix = c(".hd", ".gii"))
names(human)
dim(human)
library(stringr)
human$gni
human$gni_num <- mutate(human, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni_num
library(plyr)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(gii)
sum_hd <- summary(hd)
sum_gii <- summary(gii)
sum_hd
hdnew <- rename(hd, c("HDI.Rank"="rank", "Country"="country", "Human.Development.Index..HDI."="hdi", "Life.Expectancy.at.Birth"="lifeexp", "Expected.Years.of.Education"="eduexp", "Mean.Years.of.Education"="edumean", "Gross.National.Income..GNI..per.Capita"="gni", "GNI.per.Capita.Rank.Minus.HDI.Rank"="gnirank"))
sum_gii
str(gii)
names(hdnew)
dim(hd)
giinew <- rename(gii, c("GII.Rank"="rank", "Country"="country", "Gender.Inequality.Index..GII."="gii", "Maternal.Mortality.Ratio"="matmort", "Adolescent.Birth.Rate"="ado_birth", "Percent.Representation.in.Parliament"="rep_parl", "Population.with.Secondary.Education..Female."="sec_ed_fem", "Population.with.Secondary.Education..Male."="sec_ed_mal", "Labour.Force.Participation.Rate..Female."="labour_fem", "Labour.Force.Participation.Rate..Male."="labour_mal"))
names(giinew)
giinew <- mutate(giinew, sec_ed_F2M = sec_ed_fem/sec_ed_mal, labour_F2M = labour_fem/labour_mal)
join_by <- c("country")
library(dplyr)
human <- inner_join(hdnew, giinew, by = c(join_by), suffix = c(".hd", ".gii"))
names(human)
dim(human)
human
human$gni <- mutate(human$gni, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human <- mutate(human$gni, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
str(human$gni)
as.factor(human$gni)
str(human$gni)
human$gni <- as.factor(human$gni)
str(human$gni)
human$gni
human$gni <- mutate(human$gni, str_replace(human$gni, pattern=",", replace ="") %>% as.numeric)
human$gni <- str_replace(human$gni, pattern=",", replace ="") %>% as.numeric
human$gni
library(plyr)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(hd)
dim(gii)
sum_hd <- summary(hd)
str(gii)
sum_gii <- summary(gii)
sum_hd
sum_gii
hdnew <- rename(hd, c("HDI.Rank"="rank", "Country"="country", "Human.Development.Index..HDI."="hdi", "Life.Expectancy.at.Birth"="lifeexp", "Expected.Years.of.Education"="eduexp", "Mean.Years.of.Education"="edumean", "Gross.National.Income..GNI..per.Capita"="gni", "GNI.per.Capita.Rank.Minus.HDI.Rank"="gnirank"))
names(hdnew)
giinew <- rename(gii, c("GII.Rank"="rank", "Country"="country", "Gender.Inequality.Index..GII."="gii", "Maternal.Mortality.Ratio"="matmort", "Adolescent.Birth.Rate"="ado_birth", "Percent.Representation.in.Parliament"="rep_parl", "Population.with.Secondary.Education..Female."="sec_ed_fem", "Population.with.Secondary.Education..Male."="sec_ed_mal", "Labour.Force.Participation.Rate..Female."="labour_fem", "Labour.Force.Participation.Rate..Male."="labour_mal"))
names(giinew)
giinew <- mutate(giinew, sec_ed_F2M = sec_ed_fem/sec_ed_mal, labour_F2M = labour_fem/labour_mal)
join_by <- c("country")
library(dplyr)
human <- inner_join(hdnew, giinew, by = c(join_by), suffix = c(".hd", ".gii"))
names(human)
dim(human)
human
library(stringr)
human$gni <- str_replace(human$gni, pattern=",", replace ="") %>% as.numeric
human$gni
keep_columns <- c("country", "sec_ed_F2M", "labour_F2M", "eduexp", "lifeexp", "gni", "matmort", "ado_birth", "rep_parl")
human <- select(human, one_of(keep_columns))
human
names(human)
human <- filter(human, complete.cases(human) == TRUE))
human <- filter(human, complete.cases(human) == TRUE)
human$country
last <- nrow(human) - 7
human <- human[1:last, ]
human$country
dim(human)
rownames(human) <- human$country
human
human <- subset(human, select = -c("country"))
human <- subset(human, select = -c(country))
dim(human)
human
write.csv(human, file = "./data/human.csv", row.names=TRUE)
setwd("~/GitHub/IODS-project")
write.csv(human, file = "./data/human.csv", row.names=TRUE)
human_test <- read.csv(file = "./data/human.csv", row.names = TRUE)
human_test <- read.csv(file = "./data/human.csv")
identical(human, human_test)
human_test
human_test <- read.csv(file = "./data/human.csv", row.names = TRUE)
human_test <- read.csv(file = "./data/human.csv", row.names = 1)
identical(human, human_test)
human_test
compare(human, human_test)
library(compare)
install.packages("compare")
library(compare)
compare(human, human_test)
?compare
human <- read.csv(file = "./data/human.csv", row.names = 1)
dim(human)
str(human)
library(GGally)
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
library(GGally)
p <- ggpairs(human, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
library(GGally)
library(ggplot2)
p <- ggpairs(human, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
