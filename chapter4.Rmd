# Chapter 4 - Clustering and classification

```{r echo = FALSE, results='hide', message=FALSE}
library(MASS)
library(dplyr)
library(GGally)
library(ggplot2)
library(corrplot)
```

Libraries loaded, let's get clustering. First up we'll look at the structure and dimensions of the Boston data, a dataframe generously provided as part of the R package MASS.If you want to know more about this dataset have a look [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). Now a quick overview of the data:

```{r}
str(Boston)
dim(Boston)

summary(Boston$crim)

summary(Boston$tax)
```

So in brief, this data describes housing values in the suburbs of Boston. We have 14 variables and 506 observations.The variable _crim_ refers to per capita crime rate and will be our primary variable of interest. Examples of other variables are _chas_, a dichotomous variable describing whether the area bounds the river Charles or not; _rm_, the average number of rooms per dwelling in the area and _ptratio_, the ratio of pupils to teacher. Check out the link above for full details on the variables.

As is obvious from the summary above, the scales of these variables are very dissimilar (I used _crim_ and _tax_ to illustrate this). This is not a problem as long as we're looking at correlations, where they're standardized anyway, but later we'll standardize them for our other analyses.

Now for a quick look at how these variables interact:

```{r}
p <- ggpairs(Boston, mapping = aes(alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
p
```

From this, we can tell that some of the variables seem to be quite strongly related and that not very many have anything close to a normal distribution. So let's have a look at the correlation matrix to get some figures for those relationships. Below the correlation matrix I'll do a correlation plot to illustrate these figures, because I can:

```{r}
cor_matrix <- cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
```

As could be assumed from the scatterplots many of these variables are quite strongly correlated. So for example there is a correlation of .63 between the variables _crim_, which is the crime rate, and _rad_, which is the index of acessibility to radial highways. Highway robbery? Go figure.

There's a few rather obvious relationships, for example the level of industry _(indus)_ is positively correlated with the level of air pollution _(nox)_. To read more about the dangers of nitrogen oxide, see [Wikipedia](https://en.wikipedia.org/wiki/NOx).

Now we'll standardize the dataset. If I remember to do so I'll look up an easy explanation for why this needs to be done and _insert it here_. If it's still missing at the time of assessment, assume I know but forgot to update my diary...

To scale the whole dataset at once, we'll do this:

```{r}
myboston_scaled <- scale(Boston)
summary(myboston_scaled)
myboston_scaled <- as.data.frame(myboston_scaled)
```

That was easy. Now the variables all have a mean of zero and a standard deviation of one. 

Now we're to create a categorical crime rate variable using quantiles as break points. This will allow us to (arbitrarily) define crime rates as low, moderate or high. 

```{r}
cutoffs <- quantile(myboston_scaled$crim)
labels <- c("low", "mod_low", "mod_high", "high")
crim_cat <- cut(myboston_scaled$crim, breaks=cutoffs, include.lowest = TRUE, label = labels)
table(crim_cat)
```

So there we have them tabled, obiously (approximately) the same amount of observations per quantile. Oh, now we have to get rid of the old, continuous _crim_-variable and add the new one:

```{r}
myboston_scaled <- dplyr::select(myboston_scaled, -crim)
myboston_scaled <- data.frame(myboston_scaled, crim_cat)
str(myboston_scaled)
```

Looks good. Now this dataset needs to be divided into a __training__ set and a __testing__ set. To do that we'll use the function _nrow()_ to get the number of rows and then randomly select 80% of those for our training set:

```{r}
n <- nrow(myboston_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- myboston_scaled[eighty_perc,]
test <- myboston_scaled[-eighty_perc,]
```

Now it's time for some LDA. First we'll compute the LDA fit using all the variables in the dataset:

```{r}
lda.fit <- lda(crim_cat ~ ., data = train)
lda.fit
```

Now, I'm a bit slow when it comes to understanding statistics, so i had to listen to this weeks presentation many times, read the blog posts and then dig around the internet for quite some time before being able to give a satisfactory explanation of this output. To start with, the _prior probabilities_ should be the number of observations in each class divided by the number of observations in the whole datast. The _group means_ show the means for each variable in each class, and in this dataset pretty much every variable has quite different means for different classes of the target variable.

Next i our output we have our _coefficients of linear discriminants_. This is the output I had to shop  around for a proper explanation. I liked [this one](https://rstudio-pubs-static.s3.amazonaws.com/35817_2552e05f1d4e4db8ba87b334101a43da.html).

So according to the site linked above, the first discriminant function in our case (LD1) is a linear combination of the variables above with their listed loadings (to borrow a term from factor analysis) like so:
$0.08 * zn + 0.05 * indus + ... + 0.17 * medv.$

Then last, the _proportion of trace_ tells us that our first discriminant function explains 94,4% of the between group variance while LD2 and LD3 add a pitiful 5,6% combined.

Then we'll draw a biplot:

```{r}
lda.arrows <- function(x, myscale = 0.5, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crim_cat)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)

```

As we can see, the length of the arrows correspond with the size of the discriminant coefficients in the lda output above, i.e. _rad_ has the largest absolute value (for LD1) and the longest arrow, _zn_ and _nox_ have high absolute values for LD2 and LD3. What this plot also shows is that while LD1 seems to discriminate very well between high crime and the rest, LD2 helps to discriminate the other 3 groups. How to illustrate LD3 is beyond me, so onwards with some predictions. 

Next we'll save the classes from the test set as a new vector and remove them from the test set:

```{r}
correct_classes <- test$crim_cat
test <- dplyr::select(test, -crim_cat)
```

And finally, we'll use our LDA-model to predict classes for the test set and compare those to the correct classes by cross-tabulating:

```{r}
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
```

So our model predicted the crime rates rather well. A quick calculation gives us a total of
```{r}
total <- c(18+17+14+26+5+1+7+7)
total
```
observations of which
```{r}
corr <- c(18+17+14+26)
corr
```
were correctly predicted while
```{r}
wrong <- c(5+1+7+7)
wrong
```
were incorrectly predicted, resulting in a prediction accuracy of
```{r}
ratio <- c(corr/total)
ratio
```

In conclusion our model was fairly accurate in predicting crime rates.

Now it's time to look at the clustering and distances. First up we'll reload the Boston data and scale it:
```{r}
new_Boston <- Boston
str(new_Boston)
new_Boston_scaled <- scale(new_Boston) %>% as.data.frame()
class(new_Boston_scaled)
str(new_Boston_scaled)
```

Then we'll get the Euclidean distance between observations:

```{r}
dist_eu <- dist(new_Boston_scaled)
summary(dist_eu)
head(dist_eu)
```

Then we will run k-means with 4 centers, setting the seed before so we can compare results better:
```{r}
set.seed(123)
km <-kmeans(dist_eu, centers=4)
pairs(new_Boston_scaled, col = km$cluster)
```


To determine the optimal number of clusters we will calculate the total Within Cluster Sum of Squares, which is simply the squared sum of the distances between each observation in a cluster and it's centroid. We'll set a maximum number of clusters to be high enough so we can be sure our optimal number is within the range of clusters used in the calculation.

```{r}
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, twcss, type='b')
```

Based on this, it actually seems two clusters would be optimal rather than 4. So we'll run k-means with two clusters and have a look at the plots:

```{r}
km <-kmeans(dist_eu, centers=2)
pairs(new_Boston_scaled, col = km$cluster)
```